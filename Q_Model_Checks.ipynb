{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "import sklearn\n",
    "import os\n",
    "from matplotlib.pyplot import hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_stata(\"maindata.dta\", convert_categoricals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "laws_csv = pd.read_csv(\"When_Were_Laws.csv\")\n",
    "laws_csv = laws_csv[np.logical_not(np.isnan(laws_csv[\"FIPS\"]))]  # FIPS codes identify states\n",
    "laws_csv = laws_csv.drop(\"State_Name\", axis=1)  # Dropping as useless\n",
    "laws_csv = laws_csv.rename({'FIPS': 'stfips'}, axis=1) \n",
    "\n",
    "# Merging\n",
    "merged = pd.merge(laws_csv, x, on='stfips', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_merged = merged.copy()  # To allow for re-running \n",
    "\n",
    "basic_merged = basic_merged[basic_merged[\"a_age\"] <= 25]  # Can be changed later, but for now useful I think\n",
    "#age_subset = np.logical_and(np.greater_equal(basic_merged[\"a_age\"],18), np.greater_equal(19,basic_merged[\"a_age\"]))\n",
    "# 17 <= age <= 21 (maybe should be like 22)\n",
    "#basic_merged = basic_merged[age_subset]\n",
    "#print(basic_merged.shape)\n",
    "\n",
    "# Dropping states who were treated < 97 (i.e. they always had programs)\n",
    "# This is following Callaway + Sant'anna, as we cannot meaningfully \n",
    "# do any inference using those states. Although we can compare them later as a \n",
    "# robustness check, which may be interesting\n",
    "basic_merged = basic_merged[basic_merged[\"Year_Implemented\"].str.contains(\"always\")==False]  \n",
    "\n",
    "# I also drop the never states, as they may be substantively different from others, although this can be relaxed later.\n",
    "basic_merged = basic_merged.replace(\"never\", \"1000000\") \n",
    "basic_merged[\"Year_Implemented\"] = basic_merged[\"Year_Implemented\"].astype(int)  # converting to intbasic_merged = basic_merged[basic_merged[\"Year_Implemented\"].str.contains(\"never\")==False]  # Only want to look at one for now. \n",
    "\n",
    "# As we are treating >19 as the never-treated group, we set their year implemented as 1000000 >> 1999\n",
    "year_implemented_vector = basic_merged[\"Year_Implemented\"].copy()\n",
    "year_implemented_vector[basic_merged[\"under19\"] == 0] = 1000000\n",
    "basic_merged[\"group\"] = year_implemented_vector  # Equals the year you were first treated. If >=19 then treated at t = infty\n",
    "\n",
    "# Drop Arizona since they implemented late and later repealed policy\n",
    "basic_merged = basic_merged[basic_merged[\"stfips\"] != 5]\n",
    "\n",
    "# Generating list of confounders of interest, these are not necessarily optimal. \n",
    "list_of_confounders = [\"fownu18\", \"a_maritl\", \"female\" , \"povll\"]#, \"stfips\"]\n",
    "list_of_confounders += [\"anykids\", \"disability\", \"collgrad\", \"hsgrad\"] # coll + hs are extra for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Checks for Q Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = basic_merged['pubonly']\n",
    "list_of_confounders_treatment = list_of_confounders + [\"group\"]\n",
    "confounders_and_treat = basic_merged[list_of_confounders_treatment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(confounders_and_treat, outcome, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to fit model and calculate the accuracy score, mse, and baseline\n",
    "def fit_model(model):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    test_mse=mean_squared_error(y_pred, y_test)\n",
    "    score2 = model.score(x_test,y_test)\n",
    "    baseline_mse=mean_squared_error(y_train.mean()*np.ones_like(y_test), y_test)\n",
    "    \n",
    "    return test_mse, score2, baseline_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models to test\n",
    "Rf = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "KNN = KNeighborsClassifier()\n",
    "LogReg = linear_model.LogisticRegression(multi_class='ovr', solver='liblinear')\n",
    "XGBoost = sklearn.ensemble.GradientBoostingClassifier()\n",
    "\n",
    "models = [Rf, KNN, LogReg, XGBoost]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_stat = pd.DataFrame()\n",
    "model_lst = [\"RandomForest\", \"KNN\", \"LogReg\", \"XGBoost\"]\n",
    "g_ce = []\n",
    "score = []\n",
    "baseline = []\n",
    "\n",
    "for model in models:\n",
    "    x, y, z = fit_model(model)\n",
    "    g_ce.append(x)\n",
    "    baseline.append(z)\n",
    "    score.append(y)\n",
    "\n",
    "fit_stat[\"model\"] = model_lst\n",
    "fit_stat[\"test_mse\"] = g_ce\n",
    "fit_stat[\"baseline_mse\"] = baseline\n",
    "fit_stat[\"accuracy_score\"] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>baseline_mse</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.110675</td>\n",
       "      <td>0.098426</td>\n",
       "      <td>0.889325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.116026</td>\n",
       "      <td>0.098426</td>\n",
       "      <td>0.883974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.110675</td>\n",
       "      <td>0.098426</td>\n",
       "      <td>0.889325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.100619</td>\n",
       "      <td>0.098426</td>\n",
       "      <td>0.899381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model  test_mse  baseline_mse  accuracy_score\n",
       "0  RandomForest  0.110675      0.098426        0.889325\n",
       "1           KNN  0.116026      0.098426        0.883974\n",
       "2        LogReg  0.110675      0.098426        0.889325\n",
       "3       XGBoost  0.100619      0.098426        0.899381"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE of fit model 0.11067475131087759\n",
      "Model Score: 0.8893252486891224\n"
     ]
    }
   ],
   "source": [
    "Q1 = RandomForestClassifier(n_estimators=100, max_depth=2)\n",
    "Q1.fit(x_train, y_train)\n",
    "y_pred = Q1.predict(x_test)\n",
    "\n",
    "test_mse=mean_squared_error(y_pred, y_test)\n",
    "print(f\"Test MSE of fit model {test_mse}\") \n",
    "\n",
    "score2 = Q1.score(x_test,y_test)\n",
    "print(\"Model Score: \" + str(score2)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
