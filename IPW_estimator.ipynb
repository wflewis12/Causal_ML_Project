{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Packages And Basic Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from plotnine import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing maindata\n",
    "\n",
    "file_path = \"C://Users/miste/Documents/Causal_ML/\"\n",
    "\n",
    "x = pd.read_stata(file_path + \"maindata.dta\", convert_categoricals=False)\n",
    "\n",
    "# Importing laws_csv, cleaning it\n",
    "laws_csv = pd.read_csv(\"When_Were_Laws.csv\")\n",
    "laws_csv = laws_csv[np.logical_not(np.isnan(laws_csv[\"FIPS\"]))]  # FIPS codes identify states\n",
    "laws_csv = laws_csv.drop(\"State_Name\", axis=1)  # Dropping as useless\n",
    "laws_csv = laws_csv.rename({'FIPS': 'stfips'}, axis=1) \n",
    "\n",
    "# Merging\n",
    "merged = pd.merge(laws_csv, x, on='stfips', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Datasets, only interested in the 1997 states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_merged = merged.copy()  # To allow for re-running \n",
    "\n",
    "basic_merged = basic_merged[basic_merged[\"a_age\"] <= 25]  # Can be changed later, but for now useful I think\n",
    "#age_subset = np.logical_and(np.greater_equal(basic_merged[\"a_age\"],18), np.greater_equal(19,basic_merged[\"a_age\"]))\n",
    "# 17 <= age <= 21 (maybe should be like 22)\n",
    "#basic_merged = basic_merged[age_subset]\n",
    "#print(basic_merged.shape)\n",
    "\n",
    "# Dropping states who were treated < 97 (i.e. they always had programs)\n",
    "# This is following Callaway + Sant'anna, as we cannot meaningfully \n",
    "# do any inference using those states. Although we can compare them later as a \n",
    "# robustness check, which may be interesting\n",
    "basic_merged = basic_merged[basic_merged[\"Year_Implemented\"].str.contains(\"always\")==False]  \n",
    "\n",
    "# Setting never implemented states to implementing at t=inf\n",
    "basic_merged = basic_merged.replace(\"never\", \"1000000\") \n",
    "basic_merged[\"Year_Implemented\"] = basic_merged[\"Year_Implemented\"].astype(int)  # converting to intbasic_merged = basic_merged[basic_merged[\"Year_Implemented\"].str.contains(\"never\")==False]  # Only want to look at one for now. \n",
    "\n",
    "# Drop Arizona since they implemented late and later repealed policy\n",
    "basic_merged = basic_merged[basic_merged[\"stfips\"] != 5]\n",
    "\n",
    "# As we are treating >19 as the never-treated group, we set their year implemented as 1000000 >> 1999\n",
    "year_implemented_vector = basic_merged[\"Year_Implemented\"].copy()\n",
    "year_implemented_vector[basic_merged[\"under19\"] == 0] = 1000000\n",
    "basic_merged[\"group\"] = year_implemented_vector  # Equals the year you were first treated. If >=19 then treated at t = infty\n",
    "\n",
    "# Generating list of confounders of interest, these are not necessarily optimal. \n",
    "list_of_confounders = [\"fownu18\", \"a_maritl\", \"female\" , \"povll\"]\n",
    "list_of_confounders += [\"anykids\", \"disability\", \"collgrad\", \"hsgrad\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\bold{\\text{Estimating causal effect}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_k_fold_fit_and_predict(make_model, X:pd.DataFrame, y:np.array, A:np.array, n_splits:int, year_of_interest:int):\n",
    "    '''\n",
    "    Implements K fold cross-fitting for the model predicting the outcome Y. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each set of covariates in fold j\n",
    "    3b. Each set of covariates is fitted for each combination of year x treatment\n",
    "    4. We return a matrix which contains those predictions as columns (each column is one combinatino of year x treatment)\n",
    "    5. We also return vectors which indicate the true year/treatment status for that particular set of covariates\n",
    "    \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (that implements fit and either predict_prob or predict)\n",
    "    X: dataframe of variables to adjust for\n",
    "    y: array of outcomes\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    year_of_interest: year that treatment occured\n",
    "    '''\n",
    "    # Setting up prediction vectors\n",
    "    # the 2*i'th column is year `i` with treatment = 0\n",
    "    # the (2*i)+1'th column is year `i` with treatment = 1\n",
    "    # As we predict for 18 years, there are 36 columns. \n",
    "\n",
    "    predictions_matrix = np.empty(shape=(y.shape[0], 36))\n",
    "\n",
    "    # k-folding\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # include the treatment as input feature\n",
    "    X_w_treatment = X.copy()\n",
    "    X_w_treatment[\"A\"] = A\n",
    "\n",
    "    # Generating vectors which will contain the true treatment/year combination for each covariate. \n",
    "    year_vec = np.full_like(X_w_treatment[\"year\"], np.nan, dtype=float)\n",
    "    treat_vec = np.full_like(X_w_treatment[\"year\"], np.nan, dtype=float)\n",
    "\n",
    "    for train_index, test_index in kf.split(X_w_treatment, y):\n",
    "        X_train = X_w_treatment.loc[train_index]\n",
    "        y_train = y.loc[train_index]\n",
    "        q = make_model()\n",
    "        q.fit(X_train, y_train)\n",
    "\n",
    "        # This saves the vector of years so we only get the att for the 1997 group\n",
    "        year_vec[test_index] =  X_w_treatment[\"year\"].loc[test_index]\n",
    "\n",
    "        # This saves the vector of treated statuses, as we want to condition on A = 1 (we want ATT, not ATE)\n",
    "        treat_vec[test_index] = A.loc[test_index]\n",
    "\n",
    "        for j in range(1991, 2009):\n",
    "            X0 = X_w_treatment.copy()\n",
    "            X0[\"A\"] = 1000000  # Never treated units. \n",
    "            X0[\"year\"] = j # Changing the year to the year of interest\n",
    "            col_num = 2*(j - 1991)  # Which column the data should go in\n",
    "            predictions_matrix[test_index,col_num] = q.predict_proba(X0.loc[test_index])[:, 1]\n",
    "        for j in range(1991,2009):\n",
    "            X1 = X_w_treatment.copy() \n",
    "            # We do not adjust the year here, as we are only interested in their actual treatment status\n",
    "            # That means we don't need to change it, as it is already how we want it. \n",
    "            X1[\"year\"] = j # Changing the year to the year of interest\n",
    "            col_num = 2*(j - 1991) + 1  # Which column the data should go in\n",
    "            predictions_matrix[test_index,col_num] = q.predict_proba(X1.loc[test_index])[:, 1]\n",
    "\n",
    "    assert np.isnan(predictions_matrix).sum() == 0  # Sanity check that we have no missings\n",
    "    return predictions_matrix, year_vec, treat_vec\n",
    "\n",
    "def treatment_k_fold_fit_and_predict(make_model, X:pd.DataFrame, A:np.array, n_splits:int, year_of_interest:int):\n",
    "    \"\"\"\n",
    "    k-fold predicting A given X\n",
    "\n",
    "    Inputs: \n",
    "    make_model: G_model of interest\n",
    "    X: Dataframe (always basic_merged)\n",
    "    A: vector of which group each person belongs to\n",
    "    n_splits: how much we split for the k-folding\n",
    "    year_of_interest: what group we are estimating at this point. \n",
    "    \"\"\"\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # group of people \n",
    "    G_g_and_C = np.logical_or(np.equal(A,year_of_interest), np.equal(A,1000000))\n",
    "\n",
    "    # subset for people in G_g or C, then reset index to allow for kfolding\n",
    "    covariates = X.copy()\n",
    "    covariates = covariates.loc[G_g_and_C]\n",
    "    covariates = covariates.reset_index(drop=True)\n",
    "\n",
    "    treatment_status = (A == year_of_interest)\n",
    "    treatment_status = treatment_status.loc[G_g_and_C]\n",
    "    treatment_status = treatment_status.reset_index(drop=True)\n",
    "\n",
    "    g_matrix = np.full_like(treatment_status, np.nan, dtype=float)\n",
    "\n",
    "    # Generating vectors which will contain the true treatment/year combination for each covariate. \n",
    "    # Normalizing years so that year =1 when treatment first occurs. \n",
    "    #covariates.drop(\"year\", axis=1)  # We are interested in the covariates, not the year they were found in. \n",
    "\n",
    "    for train_index, test_index in kf.split(covariates, treatment_status):\n",
    "        X_train = covariates.loc[train_index]\n",
    "        y_train = treatment_status.loc[train_index]\n",
    "        g = make_model()\n",
    "        g.fit(X_train, y_train)\n",
    "        g_matrix[test_index] = g.predict_proba(covariates.loc[test_index])[:, 1]\n",
    "    assert np.isnan(g_matrix).sum() == 0  # Sanity check that we have no missings\n",
    "    print(\"Max p:\" + str(np.max(g_matrix))) # Checking for overlap\n",
    "    return g_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the ATT's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ATT_event_study(year_of_interest:int, response:str, confounder_list:list, dataframe:pd.DataFrame, G_model, Q_model):\n",
    "    '''\n",
    "    Estimates ATT(g,t) for the combinations of g,t which we are interested in. \n",
    "\n",
    "    Based on Callaway & Sant'anna, we compute ATT(g,t) over time\n",
    "\n",
    "    Inputs: \n",
    "    year_of_interest: the year at which treatment first occurs\n",
    "    response: which outcome we are interested in, out of public/private/all insurance. \n",
    "    confounder_list: list of columns of dataframe to use as confounders\n",
    "    dataframe: dataframe to get data from\n",
    "    g_model: machine learning ensemble model to use as the g model\n",
    "    q_model : machine learning ensemble model to use as the q model\n",
    "    '''\n",
    "    # We don't subset here, as we want to fit on all data. \n",
    "    #state_level = dataframe.loc[dataframe['stfips'].isin(stfips),:] # Subset for states of interest\n",
    "    results_df = {}\n",
    "    state_level = dataframe.copy()\n",
    "    state_level = state_level[confounder_list+ [\"group\",\"year\", response]]  # Subsetting for features of interest\n",
    "\n",
    "    # Drop missings, only needed for some models, \n",
    "    state_level = state_level.dropna(axis = 0)\n",
    "\n",
    "    # Setting up \n",
    "    state_level = state_level.reset_index(drop=True)\n",
    "    confounders = state_level[list_of_confounders + [\"year\"]]\n",
    "    treatment = state_level[\"group\"]\n",
    "    outcome = state_level[response]\n",
    "\n",
    "    predictions_matrix, year_vec, treat_vec = outcome_k_fold_fit_and_predict(Q_model, \\\n",
    "                                       X=confounders, y=outcome, A=treatment, n_splits=5, \\\n",
    "                                        year_of_interest=year_of_interest)\n",
    "                                        \n",
    "    G_g_and_C = np.logical_or(np.equal(treatment,year_of_interest), np.equal(treatment,1000000))\n",
    "\n",
    "    predictions_matrix = predictions_matrix[G_g_and_C,:]\n",
    "    year_vec = year_vec[G_g_and_C]\n",
    "    treat_vec = treat_vec[G_g_and_C]\n",
    "    outcome = outcome[G_g_and_C]\n",
    "\n",
    "    # the 2*i'th column is always the untreated vec, and the (2*i)+1 is treated. \n",
    "    g_matrix = treatment_k_fold_fit_and_predict(G_model, X=confounders, A=treatment, n_splits=5, \\\n",
    "                                                year_of_interest=year_of_interest)\n",
    "                                                \n",
    "    # Weight on the first term. \n",
    "    # This depends only on g so we can pull it out here\n",
    "    w_g = (treat_vec == year_of_interest) #* (year_vec == year) # We only want the ATT, so we weight appropriately\n",
    "                                                # to only look at the untils treated at t=1\n",
    "    w_g = w_g / np.mean(w_g) # To ensure mean of the weights is 1, weights are >= 0 by construction\n",
    "    \n",
    "    for i in range(1,18): # we look at the ATT for 1992-2008 (cannot look at 1991 as we have no 1990 data)\n",
    "        year = i + 1991 # What actual year we are looking at in the for loop\n",
    "        # Generating term 1:\n",
    "        m_treat_1 = predictions_matrix[:,(2*i)+1]  # m_treat_1 \n",
    "        m_nev_1 = predictions_matrix[:,(2*i)]  # m_nev_1\n",
    "        if year >= year_of_interest: # If we've gotten to the treatment year \n",
    "            m_treat_0 = predictions_matrix[:,(2*(year_of_interest-1991))-1]  # m_treat_0\n",
    "            m_nev_0 = predictions_matrix[:,(2*(year_of_interest-1991))-2]  # m_nev_0\n",
    "        else:  # otherwise we do a short difference. \n",
    "            m_treat_0 = predictions_matrix[:,(2*i)-1]  # m_treat_0\n",
    "            m_nev_0 = predictions_matrix[:,(2*i)-2]  # m_nev_0\n",
    "        term_1 = w_g * (m_treat_1 - m_treat_0 - (m_nev_1 - m_nev_0))  \n",
    "\n",
    "        # Generating term 2:\n",
    "        # This weight equals 1 if the unit appears at time t and is in group g\n",
    "        w_treat_1 = (year_vec == year) * (treat_vec == year_of_interest)  # Equals 1 if in year, and treated at year_of_interest\n",
    "\n",
    "        w_treat_1 = w_treat_1 / np.mean(w_treat_1)\n",
    "\n",
    "        # This weight equals 1 if the unit appears at time g_d_1 and is in group g\n",
    "        if year >= year_of_interest: # If we've gotten to the treatment year\n",
    "            w_treat_0 = (year_vec == year_of_interest - 1) * (treat_vec == year_of_interest)  # Equals 1 if in year, and treated at year_of_interest\n",
    "            w_treat_0 = w_treat_0 / np.mean(w_treat_0)\n",
    "        else:\n",
    "            w_treat_0 = (year_vec == year - 1) * (treat_vec == year_of_interest)  # Equals 1 if in year, and treated at year_of_interest\n",
    "            w_treat_0 = w_treat_0 / np.mean(w_treat_0)\n",
    "\n",
    "        term_2 = w_treat_1 * (outcome - m_treat_1) - w_treat_0 * (outcome - m_treat_0)\n",
    "        \n",
    "        # Generating term 3:\n",
    "        w_nev_1 = (year_vec == year) * (treat_vec >= 2002) * g_matrix / (1 - g_matrix)\n",
    "\n",
    "        w_nev_1 = w_nev_1 / np.mean(w_nev_1)\n",
    "        \n",
    "        if year >= year_of_interest:\n",
    "            w_nev_0 = (year_vec == year_of_interest - 1) * (treat_vec >= 3000) * g_matrix / (1 - g_matrix)\n",
    "            w_nev_0 = w_nev_0 / np.mean(w_nev_0)\n",
    "        else:\n",
    "            w_nev_0 = (year_vec == year - 1) * (treat_vec >= 3000) * g_matrix / (1 - g_matrix)\n",
    "            w_nev_0 = w_nev_0 / np.mean(w_nev_0)\n",
    "        \n",
    "        term_3 = w_nev_1 * (outcome - m_nev_1) - w_nev_0 * (outcome - m_nev_0)\n",
    "\n",
    "        # Multiplying by 100 to convert to %\n",
    "        final = term_1 + term_2 - term_3\n",
    "        mean = np.mean(final) * 100\n",
    "        phi = final - np.mean(final) * (treat_vec == year_of_interest)\n",
    "        ste = np.std(phi) * 100 / np.sqrt(final.shape[0])\n",
    "\n",
    "        results_df.setdefault('year',[]).append(i + 1991)\n",
    "        results_df.setdefault('att',[]).append(mean)\n",
    "        results_df.setdefault('ste',[]).append(ste)\n",
    "        results_df.setdefault('q_model',[]).append(Q_model.__name__ )\n",
    "        results_df.setdefault('g_model',[]).append(G_model.__name__ )\n",
    "        results_df.setdefault('response',[]).append(response)\n",
    "        results_df.setdefault('treat_year',[]).append(year_of_interest)\n",
    "    \n",
    "    results_df = pd.DataFrame(results_df)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Models to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def large_randf():\n",
    "    ''' A function that returns a general ML g model for later use in k-folding'''\n",
    "    return RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)\n",
    "\n",
    "def xgb():\n",
    "    ''' A function that returns a general ML q model for later use in k-folding'''\n",
    "    return XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the ATT's for all years\n",
    "all_years = pd.DataFrame()\n",
    "\n",
    "for year in [1997, 1998, 1999]:\n",
    "    for response in [\"pubonly\",\"privonly\"]:\n",
    "        new_model = ATT_event_study(year, response, list_of_confounders, basic_merged, large_randf, xgb)\n",
    "        all_years = pd.concat([all_years, new_model], ignore_index=True)\n",
    "\n",
    "all_years['`Implementation Year`'] = all_years[\"treat_year\"].apply(lambda x: str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years.to_csv(\"./ATT_Results/all_years.csv\")\n",
    "all_years.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the pretreatment effects\n",
    "all_years_from_csv = pd.read_csv(\"ATT_Results/all_years.csv\")\n",
    "all_years_from_csv['adjusted_year'] = all_years_from_csv[\"year\"] - all_years_from_csv[\"treat_year\"]\n",
    "all_years_from_csv['`Implementation Year`'] = all_years_from_csv[\"treat_year\"].apply(lambda x: str(x))\n",
    "all_pre_treat = all_years_from_csv[(all_years_from_csv['adjusted_year'] < 0)].copy()\n",
    "all_pre_treat['att-error'] = all_pre_treat['att'] - all_pre_treat['ste']\n",
    "all_pre_treat['att+error'] = all_pre_treat['att'] + all_pre_treat['ste']\n",
    "all_pre_treat['treat_year'] = all_pre_treat['treat_year'].apply(lambda x: str(x))\n",
    "\n",
    "p = ggplot(all_pre_treat, aes(x='adjusted_year', y='att', color = '`Implementation Year`', group='`Implementation Year`')) + \\\n",
    "geom_errorbar(aes(ymin = 'att-error', ymax = 'att+error')) + \\\n",
    "geom_point() + \\\n",
    "geom_line() + \\\n",
    "facet_wrap('response') + \\\n",
    "xlab('Years Prior to Treatment') + \\\n",
    "ylab('Average Treatment Effect (percent)') + \\\n",
    "labs(title = \"Yearly Average Treatment Effect on the Treated Before Treatment \")\n",
    "\n",
    "# Save image\n",
    "p.save(filename = 'pre_treatment_AIPTW_all_cov.png', height=5, width=5, units = 'in', dpi=1000)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the post treatment effects\n",
    "all_years_from_csv = pd.read_csv(\"ATT_Results/all_years.csv\")\n",
    "all_years_from_csv['adjusted_year'] = all_years_from_csv[\"year\"] - all_years_from_csv[\"treat_year\"]\n",
    "all_years_from_csv['`Implementation Year`'] = all_years_from_csv[\"treat_year\"].apply(lambda x: str(x))\n",
    "all_pre_treat = all_years_from_csv[(all_years_from_csv['adjusted_year'] >= 0)].copy()\n",
    "all_pre_treat['att-error'] = all_pre_treat['att'] - all_pre_treat['ste']\n",
    "all_pre_treat['att+error'] = all_pre_treat['att'] + all_pre_treat['ste']\n",
    "all_pre_treat['treat_year'] = all_pre_treat['treat_year'].apply(lambda x: str(x))\n",
    "\n",
    "p = ggplot(all_pre_treat, aes(x='adjusted_year', y='att', color = '`Implementation Year`', group='`Implementation Year`')) + \\\n",
    "geom_errorbar(aes(ymin = 'att-error', ymax = 'att+error')) + \\\n",
    "geom_point() + \\\n",
    "geom_line() + \\\n",
    "facet_wrap('response') + \\\n",
    "xlab('Years After Treatment') + \\\n",
    "ylab('Average Treatment Effect (percent)') + \\\n",
    "labs(title = \"Cumulative Average Treatment Effect on the Treated After Treatment \")\n",
    "\n",
    "# Save image\n",
    "p.save(filename = 'post_treatment_AIPTW_all_cov.png', height=5, width=5, units = 'in', dpi=1000)\n",
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1117a3f67b6f9f07c9cd111c1d164c2c78fdaed6c7ec8e557fb0bc2f3980f47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
