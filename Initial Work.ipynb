{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.venv/scripts/activate  ; no source. \n",
    "import xgboost as xgb\n",
    "from matplotlib.pyplot import hist\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_SEED = 23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing maindata\n",
    "file_path = \"C://Users/miste/Documents/Causal_ML/\"\n",
    "x = pd.read_stata(file_path + \"maindata.dta\", convert_categoricals=False)\n",
    "\n",
    "# Importing laws_csv, cleaning it and merging with maindata\n",
    "laws_csv = pd.read_csv(file_path + \"When_Were_Laws.csv\")\n",
    "laws_csv = laws_csv[np.logical_not(np.isnan(laws_csv[\"FIPS\"]))]\n",
    "\n",
    "laws_csv = laws_csv.drop(\"State_Name\", axis=1)\n",
    "laws_csv = laws_csv.rename({'FIPS': 'stfips'}, axis=1) \n",
    "\n",
    "merged = pd.merge(laws_csv, x, on='stfips', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super basic model, we only look at 1997\n",
    "# We drop all of the NaN rows for the variables we are interested in. This leaves us with: 864922 observations\n",
    "\n",
    "basic_merged = merged.copy()\n",
    "\n",
    "basic_merged = basic_merged[basic_merged[\"Year_Implemented\"].str.contains(\"always\")==False]\n",
    "basic_merged = basic_merged.replace(\"never\", \"1000000\")  # Means that the indicator for year will always equal 1\n",
    "basic_merged[\"Year_Implemented\"] = basic_merged[\"Year_Implemented\"].astype(int) # converting to int\n",
    "\n",
    "basic_merged[\"year_indic\"] = (basic_merged[\"year\"] >= basic_merged[\"Year_Implemented\"])  # indicator for if treatment has occured in state i\n",
    "basic_merged[\"treatment\"] = basic_merged[\"under19\"] * basic_merged[\"year_indic\"]\n",
    "\n",
    "list_of_confounders = [ \"fownu18\", \"fpovcut\", \"povll\", \"faminctm1\", \"a_maritl\"] \n",
    "list_of_confounders += [\"a_hga\",  \"anykids\", \"year\", \"stfips\", \"disability\", \"elig\"] \n",
    "\n",
    "#list_of_confounders += [\"noemp_insured\", \"disability\"] \n",
    "# Dropping years  outside of [1995,2000] \n",
    "basic_merged = basic_merged[basic_merged[\"year\"] <= 2000]\n",
    "basic_merged = basic_merged[basic_merged[\"year\"] >= 1995]\n",
    "\n",
    "basic_merged = basic_merged[list_of_confounders + [\"treatment\", \"pubonly\", \"insured\", \"privonly\", \"Year_Implemented\"]]\n",
    "basic_merged = basic_merged.dropna(axis = 0)\n",
    "\n",
    "confounders_and_treat = basic_merged[list_of_confounders + [\"treatment\"]]\n",
    "confounders_no_treat = basic_merged[list_of_confounders]\n",
    "\n",
    "y_var = basic_merged[\"privonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 0.805837797893959\n"
     ]
    }
   ],
   "source": [
    "# Fitting Q\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(confounders_and_treat, y_var, test_size=0.2)\n",
    "\n",
    "Q = HistGradientBoostingClassifier()\n",
    "Q.fit(x_train, y_train)\n",
    "score2 = Q.score(x_test,y_test)\n",
    "print(\"Model Score: \" + str(score2))   \n",
    "                          \n",
    "# Can fit g as well, normally get ~95-99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATT(1997, 1997, 0) = \n",
    "# G_g = 1 if Year_Imp == 1997\n",
    "# E(G_g) = % of those first treated in 1997\n",
    "# Y_t = projected insured rate at time t\n",
    "# Y_{g-1} (we assume delta = 0 here) = projected insured rate at time g - 1 = 1996\n",
    "# C = 1 if never treated\n",
    "# m_nev = E(Y_t^C - Y_{g-1}^C | X, C = 1) so we want to estimate Y_t and Y_1996 for those never treated. \n",
    "\n",
    "#Then:\n",
    "# ATT(1997, 1997,0) = E[G_g / E(G_g) * (Y_t - Y_{g-1} - m_nev)]\n",
    "\n",
    "t = 1997\n",
    "base_year = 1996\n",
    "\n",
    "\n",
    "basic_merged[\"t_indic\"] = (basic_merged[\"Year_Implemented\"] == 1997)\n",
    "basic_merged[\"never_indic\"] = (basic_merged[\"Year_Implemented\"] == 1000000)\n",
    "\n",
    "\n",
    "Yt = confounders_and_treat.copy()\n",
    "Yg = confounders_and_treat.copy()\n",
    "\n",
    "Yt[\"year\"] = t\n",
    "Yg[\"year\"] = base_year\n",
    "\n",
    "\n",
    "\n",
    "Yt = Q.predict_proba(Yt)[:,1]\n",
    "Yg = Q.predict_proba(Yg)[:,1]\n",
    "\n",
    "Yt_C = confounders_and_treat.copy()\n",
    "Yg_C = confounders_and_treat.copy()\n",
    "\n",
    "Yt_C[\"year\"] = t\n",
    "Yg_C[\"year\"] = base_year\n",
    "\n",
    "Yt_C[\"treatment\"] = 0\n",
    "Yg_C[\"treatment\"] = 0\n",
    "\n",
    "Yt_C = Q.predict_proba(Yt_C)[:,1]\n",
    "Yg_C = Q.predict_proba(Yg_C)[:,1]\n",
    "\n",
    "term_1 = basic_merged[\"t_indic\"] / np.mean(basic_merged[\"t_indic\"])\n",
    "term_2 = (Yt - Yg - (Yt_C - Yg_C))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 4.8824392437633997e-05\n",
      "STE: 2.170430722799204e-05\n",
      "t: 2.2495254939381426\n"
     ]
    }
   ],
   "source": [
    "# Actually computing it now\n",
    "term_1 = basic_merged[\"t_indic\"] / np.mean(basic_merged[\"t_indic\"])\n",
    "term_2 = (Yt - Yg - (Yt_C - Yg_C))\n",
    "\n",
    "mean = np.mean(term_1 * term_2)\n",
    "ste = np.std(term_1 * term_2) / np.sqrt(Yt.shape[0])\n",
    "\n",
    "print(\"Mean: \" + str(mean))\n",
    "print(\"STE: \" + str(ste))\n",
    "\n",
    "print(\"t: \" + str( mean / ste))\n",
    "\n",
    "#so we get + .02% lmao, at least its significant. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1117a3f67b6f9f07c9cd111c1d164c2c78fdaed6c7ec8e557fb0bc2f3980f47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
