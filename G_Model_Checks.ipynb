{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "import sklearn\n",
    "import os\n",
    "from matplotlib.pyplot import hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED=42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_stata(\"maindata.dta\", convert_categoricals=False)\n",
    "laws_csv = pd.read_csv(\"When_Were_Laws.csv\")\n",
    "laws_csv = laws_csv[np.logical_not(np.isnan(laws_csv[\"FIPS\"]))]  # FIPS codes identify states\n",
    "laws_csv = laws_csv.drop(\"State_Name\", axis=1)  # Dropping as useless\n",
    "laws_csv = laws_csv.rename({'FIPS': 'stfips'}, axis=1) \n",
    "\n",
    "# Merging\n",
    "merged = pd.merge(laws_csv, x, on='stfips', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_merged = merged.copy()  # To allow for re-running \n",
    "\n",
    "basic_merged = basic_merged[basic_merged[\"a_age\"] <= 25]  # Can be changed later, but for now useful I think\n",
    "#age_subset = np.logical_and(np.greater_equal(basic_merged[\"a_age\"],18), np.greater_equal(19,basic_merged[\"a_age\"]))\n",
    "# 17 <= age <= 21 (maybe should be like 22)\n",
    "#basic_merged = basic_merged[age_subset]\n",
    "#print(basic_merged.shape)\n",
    "\n",
    "# Dropping states who were treated < 97 (i.e. they always had programs)\n",
    "# This is following Callaway + Sant'anna, as we cannot meaningfully \n",
    "# do any inference using those states. Although we can compare them later as a \n",
    "# robustness check, which may be interesting\n",
    "basic_merged = basic_merged[basic_merged[\"Year_Implemented\"].str.contains(\"always\")==False]  \n",
    "\n",
    "# I also drop the never states, as they may be substantively different from others, although this can be relaxed later.\n",
    "basic_merged = basic_merged.replace(\"never\", \"1000000\") \n",
    "basic_merged[\"Year_Implemented\"] = basic_merged[\"Year_Implemented\"].astype(int)  # converting to intbasic_merged = basic_merged[basic_merged[\"Year_Implemented\"].str.contains(\"never\")==False]  # Only want to look at one for now. \n",
    "\n",
    "# As we are treating >19 as the never-treated group, we set their year implemented as 1000000 >> 1999\n",
    "year_implemented_vector = basic_merged[\"Year_Implemented\"].copy()\n",
    "year_implemented_vector[basic_merged[\"under19\"] == 0] = 1000000\n",
    "basic_merged[\"group\"] = year_implemented_vector  # Equals the year you were first treated. If >=19 then treated at t = infty\n",
    "\n",
    "# Drop Arizona since they implemented late and later repealed policy\n",
    "basic_merged = basic_merged[basic_merged[\"stfips\"] != 5]\n",
    "\n",
    "# Generating list of confounders of interest, these are not necessarily optimal. \n",
    "list_of_confounders = [\"fownu18\", \"a_maritl\", \"female\" , \"povll\"]#, \"stfips\"]\n",
    "list_of_confounders += [\"anykids\", \"disability\", \"collgrad\", \"hsgrad\"] # coll + hs are extra for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_g_model():\n",
    "  return RandomForestClassifier(random_state = 42, n_estimators=100, max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment_k_fold_fit_and_predict(make_model, X:pd.DataFrame, A:np.array, n_splits:int):\n",
    "    \"\"\"\n",
    "    Implements K fold cross-fitting for the model predicting the treatment A. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each data point in fold j\n",
    "    Returns an array containing the predictions  \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (which implements fit and predict_prob)\n",
    "    X: dataframe of variables to adjust for\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    \"\"\"\n",
    "    predictions = np.full_like(A, np.nan, dtype=float)\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, A):\n",
    "      X_train = X.loc[train_index]\n",
    "      A_train = A.loc[train_index]\n",
    "      g = make_model()\n",
    "      g.fit(X_train, A_train)\n",
    "\n",
    "      # get predictions for split\n",
    "      predictions[test_index] = g.predict_proba(X.loc[test_index])[:, 1]\n",
    "\n",
    "    assert np.isnan(predictions).sum() == 0\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overlap and g scores are calculated as the probabiliy that the indiviual is treated (under 19) in treated year given\n",
    "#that you were either in that state in 1997 or never treated at all (over age of 19)\n",
    "def calculate_g(treated_year):\n",
    "    sub_merged = basic_merged.copy()\n",
    "    \n",
    "    #data of indiviuals \n",
    "    sub_merged = sub_merged[(sub_merged[\"group\"] == treated_year) | (sub_merged[\"group\"] == 1000000)]\n",
    "    \n",
    "    #Creating binary variablee\n",
    "    treatment_bin = {treated_year: 1, 1000000: 0}\n",
    "    sub_merged.group = [treatment_bin[item] for item in sub_merged.group]\n",
    "    sub_merged = sub_merged.reset_index()\n",
    "    \n",
    "    treatment = sub_merged[\"group\"]\n",
    "    confounders = sub_merged[list_of_confounders]\n",
    "    \n",
    "    #Predicting g for a given year\n",
    "    g = treatment_k_fold_fit_and_predict(make_g_model, X=confounders, A=treatment, n_splits=10)\n",
    "    \n",
    "    return g\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = calculate_g(1997)\n",
    "#plotting the propensity scores to check overlap\n",
    "hist(g1, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the largest propensity score\n",
    "g1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = calculate_g(1998)\n",
    "hist(g2, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = calculate_g(1999)\n",
    "hist(g3, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to loop through and different models and calculate the MSE and accuracy score for each one\n",
    "def fit_g(treated_year, g_model):\n",
    "    sub_merged = basic_merged.copy()\n",
    "    sub_merged = sub_merged[(sub_merged[\"group\"] == treated_year) | (sub_merged[\"group\"] == 1000000)]\n",
    "    \n",
    "    treatment_bin = {treated_year: 1, 1000000: 0}\n",
    "    sub_merged.group = [treatment_bin[item] for item in sub_merged.group]\n",
    "    sub_merged = sub_merged.reset_index()\n",
    "    \n",
    "    treatment = sub_merged[\"group\"]\n",
    "    confounders = sub_merged[list_of_confounders]\n",
    "    \n",
    "    x_train, x_test, a_train, a_test = train_test_split(confounders, treatment, test_size=0.2)\n",
    "    g_model.fit(x_train, a_train)\n",
    "    a_pred = g_model.predict_proba(x_test)[:,1]\n",
    "    \n",
    "    #Calculating MSE and accuracy score\n",
    "    test_ce=log_loss(a_test, a_pred)\n",
    "    baseline_ce=log_loss(a_test, a_train.mean()*np.ones_like(a_test))\n",
    "    score = g_model.score(x_test,a_test)\n",
    "    \n",
    "    return test_ce, baseline_ce, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "Rf_depth_2 = RandomForestClassifier(random_state = 42, n_estimators=100, max_depth=2)\n",
    "Rf_depth_10 = RandomForestClassifier(random_state = 42, n_estimators=100, max_depth=10)\n",
    "KNN = KNeighborsClassifier()\n",
    "LogReg = linear_model.LogisticRegression(multi_class='ovr', solver='liblinear')\n",
    "XGBoost = sklearn.ensemble.GradientBoostingClassifier()\n",
    "\n",
    "models = [Rf_depth_2, Rf_depth_10, KNN, LogReg, XGBoost]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find the fit of a g model for a given year\n",
    "def fit_year(treated_year, models):\n",
    "    fit_stat = pd.DataFrame()\n",
    "    model_lst = [\"Rf_depth2\", \"RF_depth10\", \"KNN\", \"LogReg\", \"XGBoost\"]\n",
    "    g_ce = []\n",
    "    score = []\n",
    "    baseline = []\n",
    "\n",
    "    for model in models:\n",
    "        x, y, z = fit_g(treated_year, model)\n",
    "        g_ce.append(x)\n",
    "        baseline.append(y)\n",
    "        score.append(z)\n",
    "\n",
    "    fit_stat[\"model\"] = model_lst\n",
    "    fit_stat[\"g_ce\"] = g_ce\n",
    "    fit_stat[\"baseline_ce\"] = baseline\n",
    "    fit_stat[\"accuracy_score\"] = score\n",
    "\n",
    "    return fit_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_year(1997, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_year(1999, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only testing fit for 1998\n",
    "sub_merged = basic_merged.copy()\n",
    "sub_merged = sub_merged[(sub_merged[\"group\"] == 1998) | (sub_merged[\"group\"] == 1000000)]\n",
    "    \n",
    "treatment_bin = {1998: 1, 1000000: 0}\n",
    "sub_merged.group = [treatment_bin[item] for item in sub_merged.group]\n",
    "sub_merged = sub_merged.reset_index()\n",
    "    \n",
    "treatment = sub_merged[\"group\"]\n",
    "confounders = sub_merged[list_of_confounders]\n",
    "    \n",
    "x_train, x_test, a_train, a_test = train_test_split(confounders, treatment, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_g2(g_model):\n",
    "    g_model.fit(x_train, a_train)\n",
    "    a_pred = g_model.predict_proba(x_test)[:,1]\n",
    "    \n",
    "    test_ce=log_loss(a_test, a_pred)\n",
    "    baseline_ce=log_loss(a_test, a_train.mean()*np.ones_like(a_test))\n",
    "    score = g_model.score(x_test,a_test)\n",
    "    \n",
    "    return test_ce, baseline_ce, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_stat = pd.DataFrame()\n",
    "model_lst = [\"Rf_depth2\", \"RF_depth10\", \"KNN\", \"LogReg\", \"XGBoost\"]\n",
    "g_ce = []\n",
    "score = []\n",
    "baseline = []\n",
    "\n",
    "for model in models:\n",
    "    x, y, z = fit_g2(model)\n",
    "    g_ce.append(x)\n",
    "    baseline.append(y)\n",
    "    score.append(z)\n",
    "\n",
    "fit_stat[\"model\"] = model_lst\n",
    "fit_stat[\"g_ce\"] = g_ce\n",
    "fit_stat[\"baseline_ce\"] = baseline\n",
    "fit_stat[\"accuracy_score\"] = score\n",
    "\n",
    "fit_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
